---
Theme: "[[ML]]"
---
Широко используемой в задачах обработки естественного языка (NLP).

## Основные компоненты:

- **Encoder** и **Decoder**:
  - **Encoder** принимает входные данные и обрабатывает их через несколько слоев.
  - **Decoder** генерирует выход, используя информацию от **Encoder** и предыдущие выходы.

- **Multi-Headed Attention**:
  - Позволяет модели уделять внимание разным частям входных данных одновременно.

- **Add & Norm**:
  - Операция добавления и нормализации, которая улучшает стабильность обучения.

- **Feed Forward**:
  - Линейные слои, обрабатывающие данные после внимания.

- **Linear** и **Softmax**:
  - Используются для получения вероятностей выхода, например, для предсказания следующего слова.


![[Encoder.png]]
### Применение:
Архитектура **Transformer** активно используется в моделях, таких как **BERT**, **GPT**, **T5**, для задач:
- Перевод текста.
- Генерация текста.
- Классификация текста.
- Ответы на вопросы.